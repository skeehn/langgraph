# LangGraph Advanced Performance Optimization Suite

A comprehensive performance analysis and optimization toolkit for LangGraph applications, designed to help developers identify bottlenecks, optimize execution, and improve scalability.

## üöÄ Features

### Memory Profiling
- **Real-time memory monitoring** with configurable sampling rates
- **Allocation tracking** using Python's tracemalloc
- **Memory leak detection** with node-specific analysis
- **Garbage collection efficiency** measurement
- **Memory growth pattern** analysis

### Concurrent Execution Benchmarking
- **Multi-level concurrency testing** (1, 2, 4, 8, 16, 32+ concurrent executions)
- **Multiple execution modes** (async, thread pool, process pool)
- **Scalability analysis** with performance curves
- **Bottleneck identification** in concurrent scenarios
- **Resource usage monitoring** (CPU, memory, threads)

### Real-World Workload Simulation
- **Customer Support** workload patterns
- **Code Assistant** scenarios
- **Research Agent** tasks
- **Multi-Agent Collaboration** workflows
- **Configurable arrival rates** and burst patterns
- **Performance metrics** under realistic load

### Performance Analysis
- **Comprehensive reporting** with bottleneck identification
- **Optimization recommendations** with priority scoring
- **Performance scoring** (0-100 scale)
- **Visualization generation** (memory usage, concurrency curves)
- **Baseline comparison** capabilities

### Graph Optimization
- **Automated optimization strategies**:
  - Parallel execution optimization
  - Intelligent caching
  - State compression
  - Memory optimization
  - Algorithm improvements
- **Optimization planning** with effort estimation
- **A/B testing** of optimizations
- **Performance impact measurement**

## üì¶ Installation

The performance suite is included with LangGraph. No additional installation required.

```bash
pip install langgraph
```

## üéØ Quick Start

### Basic Memory Profiling

```python
from langgraph.bench.advanced import GraphMemoryProfiler, MemoryProfiler
from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_core.messages import HumanMessage

# Create your graph
def my_node(state):
    return {"messages": [AIMessage(content="Hello!")]}

builder = StateGraph(MessagesState)
builder.add_node("my_node", my_node)
builder.add_edge(START, "my_node")
builder.add_edge("my_node", END)
graph = builder.compile()

# Profile memory usage
profiler = GraphMemoryProfiler(MemoryProfiler(track_allocations=True))
report = profiler.profile_graph_execution(
    graph, 
    {"messages": [HumanMessage(content="Test")]}
)

print(f"Peak Memory: {report.peak_memory:.2f} MB")
print(f"Memory Leaks: {len(report.memory_leaks)}")
```

### Concurrent Execution Benchmarking

```python
from langgraph.bench.advanced import ConcurrentBenchmark, ExecutionMode

# Benchmark concurrency
benchmark = ConcurrentBenchmark()
report = benchmark.benchmark_concurrency(
    graph,
    {"messages": [HumanMessage(content="Test")]},
    concurrency_levels=[1, 2, 4, 8, 16],
    execution_modes=[ExecutionMode.ASYNC, ExecutionMode.THREAD_POOL]
)

print(f"Optimal Concurrency: {report.optimal_concurrency}")
print(f"Scalability Score: {report.scalability_score:.2f}")
```

### Real-World Workload Simulation

```python
from langgraph.bench.advanced import RealWorldWorkloadSimulator, CustomerSupportWorkload

# Simulate customer support workload
simulator = RealWorldWorkloadSimulator()
workload = CustomerSupportWorkload()
profile = workload.create_profile(
    arrival_rate=2.0,  # 2 requests per second
    duration=60.0,     # 60 seconds
    burst_probability=0.1  # 10% chance of bursts
)

report = simulator.simulate_workload(graph, profile)
print(f"Success Rate: {report.successful_requests/report.total_requests:.2%}")
print(f"Avg Throughput: {report.avg_throughput:.2f} req/sec")
```

### Comprehensive Performance Analysis

```python
from langgraph.bench.advanced import PerformanceAnalyzer

# Run comprehensive analysis
analyzer = PerformanceAnalyzer()
report = analyzer.analyze_performance(
    graph,
    memory_report=memory_report,
    concurrency_report=concurrency_report,
    workload_reports=[workload_report]
)

# Print summary
analyzer.print_summary(report)

# Export detailed report
analyzer.export_report(report, "performance_report.json")
```

### Graph Optimization

```python
from langgraph.bench.advanced import GraphOptimizer, OptimizationStrategy

# Create optimization plan
optimizer = GraphOptimizer()
plan = optimizer.create_optimization_plan(graph, target_improvement=0.5)

# Apply optimizations
optimized_graph, results = optimizer.optimize_graph(
    graph, 
    plan.strategies
)

# Display results
for result in results:
    print(f"{result.strategy.value}: {result.improvement_percentage:.1f}% improvement")
```

## üñ•Ô∏è Command Line Interface

The suite includes a comprehensive CLI for easy integration into development workflows:

### Memory Profiling
```bash
python -m langgraph.bench.advanced.cli_commands memory \
    --graph my_graph.py \
    --input '{"messages": [{"role": "user", "content": "test"}]}' \
    --output memory_report.json
```

### Concurrency Benchmarking
```bash
python -m langgraph.bench.advanced.cli_commands concurrency \
    --graph my_graph.py \
    --concurrency-levels "1,2,4,8,16" \
    --execution-modes "async,thread_pool" \
    --output concurrency_report.json
```

### Workload Simulation
```bash
python -m langgraph.bench.advanced.cli_commands workload \
    --graph my_graph.py \
    --workload customer_support \
    --arrival-rate 2.0 \
    --duration 60 \
    --output workload_report.json
```

### Performance Analysis
```bash
python -m langgraph.bench.advanced.cli_commands analyze \
    --graph my_graph.py \
    --memory-report memory_report.json \
    --concurrency-report concurrency_report.json \
    --workload-reports "workload_report.json" \
    --output performance_report.json
```

### Graph Optimization
```bash
python -m langgraph.bench.advanced.cli_commands optimize \
    --graph my_graph.py \
    --strategies "caching,parallel_execution,state_compression" \
    --output optimization_report.txt
```

## üìä Performance Metrics

### Memory Metrics
- **Peak Memory Usage**: Maximum memory consumed during execution
- **Memory Growth Rate**: Rate of memory increase over time
- **Allocation Rate**: Number of memory allocations per second
- **GC Efficiency**: Garbage collection effectiveness (0-1)
- **Memory Leaks**: Detected memory leaks by node

### Concurrency Metrics
- **Throughput**: Requests processed per second
- **Latency**: Average response time
- **Scalability Score**: How well performance scales with concurrency (0-1)
- **Resource Utilization**: CPU and memory usage under load
- **Error Rate**: Percentage of failed requests

### Workload Metrics
- **Success Rate**: Percentage of successful requests
- **P95/P99 Latency**: 95th and 99th percentile response times
- **Peak Throughput**: Maximum requests per second achieved
- **Resource Efficiency**: Memory and CPU usage per request

## üîß Configuration

### Memory Profiler Configuration
```python
profiler = MemoryProfiler(
    track_allocations=True,      # Track memory allocations
    track_gc=True,              # Track garbage collection
    sample_rate=0.1,            # Sample every 100ms
    leak_threshold=1.0          # 1MB leak threshold
)
```

### Concurrent Benchmark Configuration
```python
benchmark = ConcurrentBenchmark(
    max_workers=16,             # Maximum worker threads
    warmup_runs=3,              # Warmup runs before testing
    test_runs=5,                # Test runs per concurrency level
    timeout=30.0                # Timeout per test
)
```

### Performance Analyzer Configuration
```python
analyzer = PerformanceAnalyzer(
    thresholds={
        PerformanceMetric.EXECUTION_TIME: PerformanceThreshold(
            metric=PerformanceMetric.EXECUTION_TIME,
            warning_threshold=5.0,    # 5 seconds
            critical_threshold=10.0,  # 10 seconds
            unit="seconds"
        )
    }
)
```

## üìà Optimization Strategies

### Parallel Execution
- **Automatic parallelization** of independent nodes
- **Async execution** patterns
- **Thread pool** management
- **Load balancing** across workers

### Intelligent Caching
- **Semantic similarity** based cache keys
- **Adaptive TTL** based on usage patterns
- **Cache warming** for common patterns
- **Memory-efficient** cache storage

### State Compression
- **Automatic compression** of large state objects
- **Configurable compression** algorithms
- **Decompression** on retrieval
- **Compression ratio** monitoring

### Memory Optimization
- **Streaming state updates** to reduce memory usage
- **Object pooling** for frequently created objects
- **Memory-mapped** storage for large datasets
- **Garbage collection** tuning

## üé® Visualization

The suite automatically generates performance visualizations:

- **Memory Usage Over Time**: Line chart showing memory consumption
- **Concurrency Performance Curve**: Throughput vs concurrency level
- **Workload Comparison**: Bar charts comparing different workloads
- **Performance Trends**: Historical performance data

## üß™ Testing and Validation

### Unit Tests
```bash
cd libs/langgraph/bench/advanced
python -m pytest tests/
```

### Integration Tests
```bash
python -m langgraph.bench.advanced.examples
```

### Performance Regression Testing
```bash
# Run before optimization
python -m langgraph.bench.advanced.cli_commands memory --graph my_graph.py --output baseline.json

# Run after optimization
python -m langgraph.bench.advanced.cli_commands memory --graph my_optimized_graph.py --output optimized.json

# Compare results
python -c "
import json
with open('baseline.json') as f: baseline = json.load(f)
with open('optimized.json') as f: optimized = json.load(f)
improvement = (baseline['peak_memory'] - optimized['peak_memory']) / baseline['peak_memory'] * 100
print(f'Memory improvement: {improvement:.1f}%')
"
```

## üö® Troubleshooting

### Common Issues

1. **High Memory Usage**
   - Check for memory leaks in custom nodes
   - Consider implementing state compression
   - Review object lifecycle management

2. **Poor Concurrency Performance**
   - Ensure nodes are thread-safe
   - Check for shared state modifications
   - Consider async patterns

3. **Slow Execution Times**
   - Profile individual nodes
   - Look for blocking operations
   - Consider parallel execution

4. **High Error Rates**
   - Review error handling in nodes
   - Check input validation
   - Implement retry mechanisms

### Debug Mode

Enable debug mode for detailed logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Your performance analysis code here
```

## ü§ù Contributing

We welcome contributions to the performance optimization suite! Please see our [contributing guidelines](../../../CONTRIBUTING.md) for details.

### Adding New Optimization Strategies

1. Create a new optimizer class inheriting from `BaseOptimizer`
2. Implement the required methods: `can_optimize`, `optimize`, `estimate_improvement`
3. Add the optimizer to `GraphOptimizer.optimizers`
4. Add tests and documentation

### Adding New Workload Patterns

1. Create a new workload class inheriting from `BaseWorkload`
2. Implement the `generate_request` method
3. Add the workload to `RealWorldWorkloadSimulator.workloads`
4. Add tests and examples

## üìö API Reference

### Memory Profiler
- `MemoryProfiler`: Core memory profiling functionality
- `GraphMemoryProfiler`: High-level graph memory profiler
- `MemorySnapshot`: Individual memory measurement
- `MemoryReport`: Comprehensive memory analysis

### Concurrent Benchmark
- `ConcurrentBenchmark`: Concurrency testing framework
- `ExecutionMode`: Available execution modes
- `ConcurrencyMetrics`: Individual concurrency measurements
- `ConcurrencyReport`: Concurrency analysis results

### Workload Simulation
- `RealWorldWorkloadSimulator`: Workload simulation framework
- `BaseWorkload`: Base class for workload generators
- `WorkloadRequest`: Individual workload request
- `WorkloadReport`: Workload simulation results

### Performance Analysis
- `PerformanceAnalyzer`: Comprehensive performance analysis
- `PerformanceReport`: Complete performance analysis results
- `Bottleneck`: Identified performance bottleneck
- `OptimizationRecommendation`: Performance optimization suggestion

### Graph Optimization
- `GraphOptimizer`: Main optimization engine
- `BaseOptimizer`: Base class for optimization strategies
- `OptimizationStrategy`: Available optimization strategies
- `OptimizationResult`: Individual optimization results

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](../../../LICENSE) file for details.

## üôè Acknowledgments

- Inspired by Google's Pregel paper for distributed graph computation
- Built on top of LangGraph's stateful execution framework
- Leverages Python's built-in profiling and monitoring tools
- Community feedback and contributions

---

For more information, visit the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) or join our [community forum](https://forum.langchain.com/).
